{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f417d29-8566-4ba7-af97-7966d6515198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_15648\\4075746706.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"gemma3\")\n",
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_15648\\4075746706.py:4: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = model(\"What is the capital of France?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "It's a global center for art, fashion, gastronomy, and culture. ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about Paris?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"gemma3\")\n",
    "result = model(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a99c22-a5d2-4701-b9c5-aa7a64fb4495",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m     language: \u001b[38;5;28mstr\u001b[39m = Field(description=\u001b[33m\"\u001b[39m\u001b[33mThe language the text is written in\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Structured LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m structured_llm = model.with_structured_output(Classification)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\base.py:242\u001b[39m, in \u001b[36mBaseLanguageModel.with_structured_output\u001b[39m\u001b[34m(self, schema, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Not implemented on this class.\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Implement this on child class if there is a way of steering the model to\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# generate responses that match a given schema.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
    "    )\n",
    "    language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "\n",
    "# Structured LLM\n",
    "structured_llm = model.with_structured_output(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ad9e9c-830e-4080-b5ee-8460599bcddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"\\nExtract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'Classification' function.\\n\\nPassage:\\nHe shouted angrily in Spanish!\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_prompt.format_messages(input=\"He shouted angrily in Spanish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
